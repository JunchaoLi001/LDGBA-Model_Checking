{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f1fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import manually defined automata\n",
    "%matplotlib inline\n",
    "from csrl.pomdp import GridPOMDP\n",
    "from csrl.oaa import oaa\n",
    "from csrl import ControlSynthesis\n",
    "import numpy as np \n",
    "\n",
    "oa=oaa()\n",
    "\n",
    "print('Number of Omega-automaton states (including the trap state):',oa.shape[1])\n",
    "display(oa)\n",
    "\n",
    "print('Initial state:',oa.q0)\n",
    "print('Transition function: ['),print(*['  '+str(t) for t in oa.delta],sep=',\\n'),print(']')\n",
    "print('Acceptance: ['),print(*['  '+str(t) for t in oa.acc],sep=',\\n'),print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POMDP Description\n",
    "shape = (10,10)\n",
    "# E: Empty, T: Trap, B: Obstacle\n",
    "structure = np.array([\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'B',  'B',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'B',  'B',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E'],\n",
    "['E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E',  'E']\n",
    "])\n",
    "\n",
    "# Labels of the states\n",
    "label = np.array([\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       ('b',),       ('b',)],\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       ('b',),       ('b',)],\n",
    "[(),       (),       ('c',),       ('c',),       (),       (),       (),       (),       (),       ()],\n",
    "[(),       (),       ('c',),       ('c',),       (),       (),       (),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[(),       (),       (),       (),       (),       (),       ('c',),       ('c',),       (),       ()],\n",
    "[(),       (),       (),       (),       (),       (),       ('c',),       ('c',),       (),       ()],\n",
    "[('a',),       ('a',),       (),       (),       (),       (),       (),       (),       (),       ()],\n",
    "[('a',),       ('a',),       (),       (),       (),       (),       (),       (),       (),       ()]\n",
    "],dtype=np.object)\n",
    "\n",
    "grid_pomdp = GridPOMDP(shape=shape,structure=structure,label=label)\n",
    "\n",
    "# Construct the product POMDP\n",
    "csrl = ControlSynthesis(grid_pomdp,oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33501f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csrl.belief_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331dd23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csrl.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafe01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csrl.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59fe3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csrl.train_DQN(EPISODES=10000,num_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b45f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a2d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62dbad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################### verify the dqn_cnn policy ###################\n",
    "\n",
    "from dqn_cnn import DQNAgent\n",
    "EPISODES=10\n",
    "num_steps=100\n",
    "\n",
    "Path = []\n",
    "\n",
    "gamma=0.99999\n",
    "gammaB=0.9\n",
    "\n",
    "# the defined belief_state size and action size\n",
    "belief_state_size = np.shape(csrl.belief_state)\n",
    "\n",
    "# find the size of belief_state for np.reshape\n",
    "prod_b_state_size = 1\n",
    "for i in range(len(belief_state_size)):\n",
    "    prod_b_state_size = prod_b_state_size * belief_state_size[i]\n",
    "\n",
    "# action size\n",
    "#prod_action_size = csrl.shape[4]\n",
    "prod_action_size = 4\n",
    "\n",
    "agent = DQNAgent(prod_b_state_size, csrl.shape[2], csrl.shape[3], csrl.shape[1], prod_action_size, gamma, gammaB)\n",
    "agent.load(\"./save/DQN_CNN_10_frontier.h5\")\n",
    "agent.epsilon = 0\n",
    "done = False\n",
    "num_episode_for_reward = 100 # print the accumulated reward per num of episode\n",
    "# initialize the list for plot\n",
    "accumulated_rewards=[]\n",
    "exploration_rate=[]\n",
    "accumulated_rewards_hundred_steps = []\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    accumulated_rewards_per_episode=0\n",
    "    done = False\n",
    "    subpath = []\n",
    "\n",
    "    pomdp_state = csrl.pomdp.random_state()\n",
    "    while csrl.pomdp.label[pomdp_state[0],pomdp_state[1]] == ('c',) or csrl.pomdp.structure[(pomdp_state[0],pomdp_state[1])]=='B':\n",
    "        pomdp_state = csrl.pomdp.random_state()\n",
    "    pomdp_state = (9, 7) # edit here to specify the start state or comment it out\n",
    "    state = (csrl.shape[0]-1,csrl.oa.q0)+(pomdp_state) # select the start product state\n",
    "    print('START STATE: '+str(state))\n",
    "    belief_state = csrl.belief_state # initialize the belief state\n",
    "    csrl.track = [0,1] #self.initial_track # initialize frontier set = [0,1]\n",
    "    reshaped_reward = csrl.reshaped_reward_init\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        print(state)\n",
    "        subpath.append(state)\n",
    "        # reshape the belief state as the input to acquire the action\n",
    "        input_b_state = np.reshape(belief_state,(1, csrl.shape[2], csrl.shape[3], csrl.shape[1]))\n",
    "        print('state: '+str(state))\n",
    "        # verify the existence of action and select the action from the belief_state\n",
    "        action_probs = agent.act_trained(input_b_state)\n",
    "        action_probs = np.reshape(action_probs,(prod_action_size,1))\n",
    "        i = 0\n",
    "        possible_actions = []\n",
    "        for i in range(len(csrl.A[state])):\n",
    "            possible_actions.append(action_probs[csrl.A[state][i]])\n",
    "        action = csrl.A[state][np.argmax(possible_actions)]\n",
    "        print('action selected: '+str(action))\n",
    "\n",
    "        ################## POMDP simualtion ##################\n",
    "\n",
    "        # agent moves to the next state\n",
    "        states, probs = csrl.transition_probs[state][action]\n",
    "        next_state = states[np.random.choice(len(states),p=probs)]\n",
    "        # find the observation states' list and the corresponding probabilities\n",
    "        obsv_states, obsv_probs = csrl.pomdp.get_observation_prob(next_state[-2:])\n",
    "        # observe the next state\n",
    "        obsv_state = csrl.pomdp.generate_obsv_state(obsv_states, obsv_probs)\n",
    "\n",
    "        ################## The belief_state update with the loops ##################\n",
    "\n",
    "        # temproraily store the current belief state\n",
    "        current_belief_state = belief_state\n",
    "        # belief state update\n",
    "        belief_state_after_transition = []\n",
    "        for s in csrl.states():\n",
    "            belief_state_after_transition.append(belief_state[s]*csrl.belief_transition_probs[s][action])\n",
    "        belief_state_after_transition = sum(belief_state_after_transition)\n",
    "        # update the belief state with the observation probability matrix\n",
    "        updated_belief_state = belief_state_after_transition\n",
    "        for s in csrl.states():\n",
    "            updated_belief_state[s] = updated_belief_state[s]*csrl.belief_observation_probs[s][obsv_state[0], obsv_state[1]]\n",
    "        belief_state = updated_belief_state/sum(sum(sum(sum(updated_belief_state))))\n",
    "\n",
    "        ################## Test the DQN model ##################\n",
    "        \n",
    "        # Update the frontier set + update reward setup accordingly\n",
    "        reshaped_reward = csrl.Tf(state, next_state, reshaped_reward)\n",
    "        # calculate the reward from the next belief state and find gamm\n",
    "        reward = np.sum(np.reshape(belief_state,(1,prod_b_state_size))*reshaped_reward)\n",
    "        # assign the next state\n",
    "        state = next_state\n",
    "        # record the accumulated rewards\n",
    "        accumulated_rewards_per_episode = accumulated_rewards_per_episode + reward\n",
    "        print('reward: '+str(reward))\n",
    "    \n",
    "    Path.append(subpath) # append the apth\n",
    "    \n",
    "    print('accumulated_rewards_per_episode: '+str(accumulated_rewards_per_episode))\n",
    "    accumulated_rewards.append(accumulated_rewards_per_episode)\n",
    "    exploration_rate.append(agent.epsilon)\n",
    "\n",
    "    if len(accumulated_rewards)>=num_episode_for_reward:\n",
    "        accumulated_rewards_hundred_steps.append(np.average(accumulated_rewards))\n",
    "        accumulated_rewards = []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "t1 = np.arange(0, EPISODES, 1)\n",
    "t2 = np.arange(0, len(accumulated_rewards_hundred_steps)*num_episode_for_reward, num_episode_for_reward)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "display(plt.plot(t1, exploration_rate))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "display(plt.plot(t2, accumulated_rewards_hundred_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a00d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Plot the path ###############\n",
    "import pylab as pl\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "size_x = 10\n",
    "size_y = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path[1]\n",
    "\n",
    "### plot the path on 'q0'.\n",
    "\n",
    "x = np.linspace(0,size_x,size_x+1)\n",
    "y = np.linspace(0,size_y,size_y+1) \n",
    "\n",
    "pl.figure(figsize=(10,10))\n",
    "\n",
    "hlines = np.column_stack(np.broadcast_arrays(x[0], y, x[-1], y))\n",
    "vlines = np.column_stack(np.broadcast_arrays(x, y[0], x, y[-1]))\n",
    "lines = np.concatenate([hlines, vlines]).reshape(-1, 2, 2)\n",
    "line_collection = LineCollection(lines, color=\"black\", linewidths=1)\n",
    "ax = pl.gca()\n",
    "ax.add_collection(line_collection)\n",
    "ax.set_xlim(int(x[0]), int(x[-1]))\n",
    "ax.set_ylim(int(y[0]), int(y[-1]))\n",
    "# backgroud color\n",
    "ax.add_patch(pl.Rectangle((0, 0), size_x, size_y, fill=True, color='green', alpha=.1))\n",
    "\n",
    "# plot the Blocks\n",
    "b_start_x = b_start_y = 4\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.5))\n",
    "\n",
    "# plot the Traps, 'c'\n",
    "b_start_x = 2\n",
    "b_start_y = 6\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='orange', alpha=.8))\n",
    "\n",
    "b_start_x = 6\n",
    "b_start_y = 2\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='orange', alpha=.8))\n",
    "\n",
    "# plot the 'a's\n",
    "b_start_x = b_start_y = 0\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='blue', alpha=.5))\n",
    "\n",
    "# plot the 'b's\n",
    "b_start_x = b_start_y = 8\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='green', alpha=.5))    \n",
    "        \n",
    "for i in range(len(path)):\n",
    "    if path[i][1]<=0:\n",
    "        state_idx = path[i]\n",
    "        ## convert state index in Julia to 'x,y' coordinates in python\n",
    "        \n",
    "        coord_x = path[i][3]\n",
    "        coord_y = size_x-1-path[i][2]\n",
    "        \n",
    "        ax.add_patch(pl.Circle((coord_x+0.5, coord_y+0.5), 0.2, fill=True, color='red', alpha=.4))\n",
    "        if i==0:\n",
    "            # start point\n",
    "            ax.add_patch(pl.Circle((coord_x+0.5, coord_y+0.5), 0.4, fill=True, color='purple', alpha=.9))\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the path on 'q1'.\n",
    "\n",
    "x = np.linspace(0,size_x,size_x+1)\n",
    "y = np.linspace(0,size_y,size_y+1) \n",
    "\n",
    "pl.figure(figsize=(10,10))\n",
    "\n",
    "hlines = np.column_stack(np.broadcast_arrays(x[0], y, x[-1], y))\n",
    "vlines = np.column_stack(np.broadcast_arrays(x, y[0], x, y[-1]))\n",
    "lines = np.concatenate([hlines, vlines]).reshape(-1, 2, 2)\n",
    "line_collection = LineCollection(lines, color=\"black\", linewidths=1)\n",
    "ax = pl.gca()\n",
    "ax.add_collection(line_collection)\n",
    "ax.set_xlim(int(x[0]), int(x[-1]))\n",
    "ax.set_ylim(int(y[0]), int(y[-1]))\n",
    "# backgroud color\n",
    "ax.add_patch(pl.Rectangle((0, 0), size_x, size_y, fill=True, color='green', alpha=.1))\n",
    "\n",
    "# plot the Blocks\n",
    "b_start_x = b_start_y = 4\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.5))\n",
    "\n",
    "# plot the Traps, 'c'\n",
    "b_start_x = 2\n",
    "b_start_y = 6\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='orange', alpha=.8))\n",
    "\n",
    "b_start_x = 6\n",
    "b_start_y = 2\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='orange', alpha=.8))\n",
    "\n",
    "# plot the 'a's\n",
    "b_start_x = b_start_y = 0\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='blue', alpha=.5))\n",
    "\n",
    "# plot the 'b's\n",
    "b_start_x = b_start_y = 8\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='green', alpha=.5))    \n",
    "\n",
    "ii=i\n",
    "for i in range(ii, len(path)):\n",
    "    if path[i][1]==1:\n",
    "        state_idx = path[i]\n",
    "        ## convert state index in Julia to 'x,y' coordinates in python\n",
    "        coord_x = path[i][3]\n",
    "        coord_y = size_x-1-path[i][2]\n",
    "        \n",
    "        ax.add_patch(pl.Circle((coord_x+0.5, coord_y+0.5), 0.2, fill=True, color='red', alpha=.4))\n",
    "        if i==0:\n",
    "            # start point\n",
    "            ax.add_patch(pl.Circle((coord_x+0.5, coord_y+0.5), 0.4, fill=True, color='purple', alpha=.9))\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot again the path on 'q0'.\n",
    "\n",
    "x = np.linspace(0,size_x,size_x+1)\n",
    "y = np.linspace(0,size_y,size_y+1) \n",
    "\n",
    "pl.figure(figsize=(10,10))\n",
    "\n",
    "hlines = np.column_stack(np.broadcast_arrays(x[0], y, x[-1], y))\n",
    "vlines = np.column_stack(np.broadcast_arrays(x, y[0], x, y[-1]))\n",
    "lines = np.concatenate([hlines, vlines]).reshape(-1, 2, 2)\n",
    "line_collection = LineCollection(lines, color=\"black\", linewidths=1)\n",
    "ax = pl.gca()\n",
    "ax.add_collection(line_collection)\n",
    "ax.set_xlim(int(x[0]), int(x[-1]))\n",
    "ax.set_ylim(int(y[0]), int(y[-1]))\n",
    "# backgroud color\n",
    "ax.add_patch(pl.Rectangle((0, 0), size_x, size_y, fill=True, color='green', alpha=.1))\n",
    "\n",
    "# plot the Blocks\n",
    "b_start_x = b_start_y = 4\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='black', alpha=.5))\n",
    "\n",
    "# plot the Traps, 'c'\n",
    "b_start_x = 2\n",
    "b_start_y = 6\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='orange', alpha=.8))\n",
    "\n",
    "b_start_x = 6\n",
    "b_start_y = 2\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='orange', alpha=.8))\n",
    "\n",
    "# plot the 'a's\n",
    "b_start_x = b_start_y = 0\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='blue', alpha=.5))\n",
    "\n",
    "# plot the 'b's\n",
    "b_start_x = b_start_y = 8\n",
    "b_size_x = b_size_y = 2\n",
    "ax.add_patch(pl.Rectangle((b_start_x, b_start_y), b_size_x, b_size_y, fill=True, color='green', alpha=.5))    \n",
    "\n",
    "ii=i\n",
    "for i in range(ii, len(path)):\n",
    "    if path[i][1]<=0:\n",
    "        state_idx = path[i]\n",
    "        ## convert state index in Julia to 'x,y' coordinates in python\n",
    "        coord_x = path[i][3]\n",
    "        coord_y = size_x-1-path[i][2]\n",
    "        \n",
    "        ax.add_patch(pl.Circle((coord_x+0.5, coord_y+0.5), 0.2, fill=True, color='red', alpha=.4))\n",
    "        if i==0:\n",
    "            # start point\n",
    "            ax.add_patch(pl.Circle((coord_x+0.5, coord_y+0.5), 0.4, fill=True, color='purple', alpha=.9))\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e68d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
